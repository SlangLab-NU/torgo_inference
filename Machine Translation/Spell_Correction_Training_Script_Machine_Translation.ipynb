{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Torgo Speakers spelling correction training script using machine translation**"
      ],
      "metadata": {
        "id": "sMEB5rsjI6uE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Objective: Spelling correction training for Torgo dataset speakers using machine translation**"
      ],
      "metadata": {
        "id": "qe3iGmTkJH9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ensure that GPU and RAM is set up: will be needed for training purpose**"
      ],
      "metadata": {
        "id": "Lbclr67NJUsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ot09SAYaJPXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure enough memory present so that training does not stop\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "wl__YWl7JaOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install the libraries**"
      ],
      "metadata": {
        "id": "PkDio85LJdnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install datasets\n",
        "!pip install transformers==4.28.0\n",
        "!pip install accelerate\n",
        "!pip install jiwer\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "J98RPhHJJey5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import libraries**"
      ],
      "metadata": {
        "id": "m2GhQPiRJlEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from transformers import BartTokenizerFast, BartForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from jiwer import wer\n",
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "hg5NZ9XKJnZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Hugging Face\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "lueD6QkXJrLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mount the json files from Google Drive**"
      ],
      "metadata": {
        "id": "37iR07xwJvmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount other_speakers.json file is stored in Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k1dFIUZxJzbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from the JSON files\n",
        "dataset = load_dataset('json', data_files={'train': '/content/drive/MyDrive/f01_other_speakers.json',\n",
        "                                           'test': '/content/drive/MyDrive/speaker_f01.json'})"
      ],
      "metadata": {
        "id": "ILl7EmpVJ2gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "Cd191GrQJ4nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][:5])\n",
        "print(dataset['test'][:5])"
      ],
      "metadata": {
        "id": "oRDuO7WQJ72P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By setting a fixed random seed, the data will be split into training and validation sets in a consistent manner each time the code is executed. This is useful for debugging, testing, and comparing different runs of the code. The choice of the number 42 as the seed is arbitrary and can be any integer value. The important aspect is to use the same seed consistently if reproducibility is desired."
      ],
      "metadata": {
        "id": "_WgIU-lLJ-qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and validation sets\n",
        "train_dataset, val_dataset = train_test_split(dataset['train'], test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "JdBzjzCSJ_is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_data = Dataset.from_dict(train_dataset)  # Convert the train data to a dataset\n",
        "val_data = Dataset.from_dict(val_dataset)      # Convert the validation data to a dataset"
      ],
      "metadata": {
        "id": "IaNPDEpWKCBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_data))\n",
        "print(type(val_data))"
      ],
      "metadata": {
        "id": "SltaVQXmKEMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check column names in train dataset\n",
        "print(train_data.column_names)\n",
        "\n",
        "# Check column names in validation dataset\n",
        "print(val_data.column_names)"
      ],
      "metadata": {
        "id": "bftpWO3BKHfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data['actual'][:5])\n",
        "print(train_data['prediction'][:5])\n",
        "#print(val_data[:5])"
      ],
      "metadata": {
        "id": "PfjrjzoyKIFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the source and target language columns\n",
        "source_lang = 'prediction'\n",
        "target_lang = 'actual'"
      ],
      "metadata": {
        "id": "rTQVHIXQKJ93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(source_lang)"
      ],
      "metadata": {
        "id": "aTM1NgUfKLv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the max_length for padding and truncation\n",
        "max_length = 512"
      ],
      "metadata": {
        "id": "oTtGNn_iKNmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preprocessing function serves to prepare the data for training or evaluation. It uses a tokenizer to tokenize the inputs and labels, formats the inputs by adding a source language identifier, encodes the tokenized inputs and labels, and creates a dictionary of model inputs. The function ensures that the data is properly tokenized, formatted, and encoded according to the model's requirements. It helps maintain consistency and compatibility between the input data and the model during training or evaluation."
      ],
      "metadata": {
        "id": "x4BE_O4vKSco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-base')\n",
        "\n",
        "# Tokenize the data\n",
        "# The preprocess_function function is defined to preprocess the data by tokenizing the inputs and labels\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f'{source_lang}: {text}' for text in examples[source_lang]]\n",
        "    targets = examples[target_lang]\n",
        "    encoding = tokenizer(inputs, padding=True, truncation=True, return_tensors='pt', max_length=max_length)\n",
        "    model_inputs = {\n",
        "        'input_ids': encoding['input_ids'].squeeze(),\n",
        "        'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "        'labels': tokenizer(targets, padding=True, truncation=True, return_tensors='pt')['input_ids'].squeeze()\n",
        "    }\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "sy1MwI8pKUAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a random data point from the train dataset\n",
        "sample_data = train_data[0]\n",
        "\n",
        "# Call the preprocess function on the sample data\n",
        "processed_data = preprocess_function(sample_data)\n",
        "\n",
        "# Inspect the output\n",
        "print(processed_data)"
      ],
      "metadata": {
        "id": "lmslYY6XKXhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data = preprocess_function(train_data)\n",
        "#val_data = preprocess_function(val_data)\n",
        "\n",
        "# Apply preprocess_function to train_data and val_data\n",
        "train_data = train_data.map(preprocess_function, batched=True)\n",
        "val_data = val_data.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "8HJmgBStKczF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access a few samples from train_data\n",
        "for i in range(5):\n",
        "    sample_input_ids = train_data['input_ids'][i]\n",
        "    sample_attention_mask = train_data['attention_mask'][i]\n",
        "    sample_labels = train_data['labels'][i]\n",
        "\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(\"Input IDs:\", sample_input_ids)\n",
        "    print(\"Attention Mask:\", sample_attention_mask)\n",
        "    print(\"Labels:\", sample_labels)\n",
        "    print()"
      ],
      "metadata": {
        "id": "eKPBoyVsKfWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A data loader is a component used in machine learning frameworks, such as PyTorch, to handle the loading and batching of data during the training or evaluation process. Its main purpose is to efficiently provide batches of data to the model for processing."
      ],
      "metadata": {
        "id": "z4Xb4Vd0Ki4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train the model**"
      ],
      "metadata": {
        "id": "UcBXXU8oKjr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of a data collator is to take a list of samples from a dataset and collate them into a batch that can be processed by the model during training or evaluation. It works closely with the data loader to handle the specific requirements of the model's input format."
      ],
      "metadata": {
        "id": "UoTBw49YKpan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a data_collator function for batch processing\n",
        "def data_collator(features):\n",
        "    batch = {}\n",
        "    # Pad input_ids and attention_mask to the maximum length within the batch\n",
        "    max_length = max(len(feature['input_ids']) for feature in features)\n",
        "    batch['input_ids'] = torch.stack([torch.tensor(feature['input_ids'] + [tokenizer.pad_token_id] * (max_length - len(feature['input_ids']))) for feature in features])\n",
        "    batch['attention_mask'] = torch.stack([torch.tensor(feature['attention_mask'] + [0] * (max_length - len(feature['attention_mask']))) for feature in features])\n",
        "    batch['labels'] = torch.stack([torch.tensor(feature['labels'] + [-100] * (max_length - len(feature['labels']))) for feature in features])\n",
        "    return batch"
      ],
      "metadata": {
        "id": "Icay4k2aKsfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data loader is responsible for loading and batching the data, while the data collator is responsible for formatting and aligning the data within each batch. They serve different functions in the training process."
      ],
      "metadata": {
        "id": "hKKlEUVPKvUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"spell_correction_F01\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=30,\n",
        "    predict_with_generate=True,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "A3y86poiKwPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify passing the correct inputs to the trainer\n",
        "print(\"Train Dataset:\", train_data)\n",
        "print(\"Validation Dataset:\", val_data)\n",
        "print(\"Tokenizer:\", tokenizer)\n",
        "print(\"Training Arguments:\", training_args)"
      ],
      "metadata": {
        "id": "bTuyLU5dKyiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model is initialized with the BARTForConditionalGeneration class and moved to the GPU if available.\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "msXiR0MGK0eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN THE CODE BLOCK\n",
        "# code to keep google collab running:\n",
        "# Right click -> Inspect -> Console -> paste the code below\n",
        "function ClickConnect(){\n",
        "  console.log(\"Clicking on the page\");\n",
        "  document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(ClickConnect, 60000) // Clicks every 1 minute"
      ],
      "metadata": {
        "id": "ECUN0OnPK5wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Seq2SeqTrainer is created with the defined model, training arguments, datasets, tokenizer, and data_collator\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "RBrm4JHDK6YT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}